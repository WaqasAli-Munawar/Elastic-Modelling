{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d784c614",
   "metadata": {},
   "source": [
    "y = ao + a1x1 + a2x2-------+ anxn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75016d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sql_script <- paste(scan(\"~/elastic_dfsql\",what=\"\",sep=\"\\n\",blank.lines.skip=FALSE),collapse=\"\\n\")\n",
    "\n",
    "with open(\"~/elastic_dfsql\", \"r\") as f:\n",
    "    sql_script = f.read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11495a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df <- dbGetQuery(myconn,sql_script, stringsAsFactors=F)\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Establish the database connection\n",
    "engine = create_engine('postgresql://username:password@hostname/database_name')\n",
    "\n",
    "# Execute the query and store the results in a dataframe\n",
    "df = pd.read_sql_query(sql_script, con=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b910fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disconnect from the Server\n",
    "# dbDisconnect(myconn)\n",
    "\n",
    "# Close the database connection\n",
    "engine.dispose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7328913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cal_wk <- c(\"SELECT distinct fisc_wk_id, fisc_wk_strt_dt, fisc_qtr_strt_dt\")\n",
    "\n",
    "cal_wk = \"\"\"SELECT DISTINCT fisc_wk_id, fisc_wk_strt_dt, fisc_qtr_strt_dt\n",
    "FROM your_table\n",
    "ORDER BY fisc_wk_id ASC;\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f660d467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sql_script <- paste(cal_wk,collapse=\"\\n\")\n",
    "\n",
    "sql_script = cal_wk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5362c35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the SQL code\n",
    "# cal_wk <- dbGetQuery(myconn,\n",
    "#                      sql_script, stringsAsFactors=F)\n",
    "\n",
    "\n",
    "\n",
    "# Disconnect from the Server\n",
    "# dbDisconnect(myconn)\n",
    "\n",
    "\n",
    "# Execute the query and store the results in a dataframe\n",
    "cal_wk = pd.read_sql_query(sql_script, con=engine)\n",
    "\n",
    "# Close the database connection\n",
    "engine.dispose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfb3dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scope_data1 <- df%>%  left_join (cal_wk) \n",
    "\n",
    "scope_data1 = df.merge(cal_wk, left_on = \"cal_mo_id\", right_on =\"fisc_wk_id\", how = \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ec2cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data <- read_xlsx(paste0(home_folder,'chicken_db.xlsx'))\n",
    "# data <- na.omit(data)\n",
    "\n",
    "# Read the Excel file into a dataframe\n",
    "data = pd.read_excel(home_folder + 'chicken_db.xlsx')\n",
    "\n",
    "# Remove rows with missing values\n",
    "data = data.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2512a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert \"month\" column to date format\n",
    "# data$month <- as.yearmon(data$month, \"%Y %b\")\n",
    "\n",
    "# Convert date format to \"yyyymm\" format\n",
    "# data$month <- format(data$month, \"%Y%m\")\n",
    "\n",
    "data[\"month\"] = pd.to_datetime(data[\"month\"], format = \"%Y %b\").dt.strftime(\"%Y%m\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49708cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data <- data %>% rename(cal_mo_id = month) %>% mutate(cal_mo_id = as.integer(cal_mo_id))\n",
    "\n",
    "# Rename the \"month\" column to \"cal_mo_id\"\n",
    "data = data.rename(columns={'month': 'cal_mo_id'})\n",
    "\n",
    "# Convert the \"cal_mo_id\" column to integer\n",
    "data['cal_mo_id'] = data['cal_mo_id'].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facf34e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scope_data <- scope_data1 %>%\n",
    "#   left_join(data %>% select(cal_mo_id, value)) %>% \n",
    "#   mutate(\n",
    "#     month = month(fisc_wk_strt_dt),\n",
    "#     year = year(fisc_wk_strt_dt),\n",
    "#     week_of_month = week(fisc_wk_strt_dt))%>%\n",
    "#   group_by(fisc_wk_strt_dt,month, year, week_of_month, mkt_lvl,value) %>%\n",
    "#   summarize(total_wgt = sum(total_wgt, na.rm = T),\n",
    "#             total_sales = sum(total_sales, na.rm = T),\n",
    "#             avg_unit_price = total_sales/total_wgt) %>% \n",
    "#   mutate(avg_unit_price_adj = (avg_unit_price/value)*100 ,\n",
    "#          log_price = log(avg_unit_price_adj),\n",
    "#          log_units = log(total_wgt)) %>%\n",
    "  \n",
    "#   ungroup()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Merge the \"scope_data1\" and \"data\" dataframes on \"cal_mo_id\"\n",
    "scope_data = scope_data1.merge(data[['cal_mo_id', 'value']], on='cal_mo_id')\n",
    "\n",
    "# Convert \"fisc_wk_strt_dt\" to \"month\", \"year\", and \"week_of_month\" columns\n",
    "scope_data['fisc_wk_strt_dt'] = pd.to_datetime(scope_data['fisc_wk_strt_dt'])\n",
    "scope_data['month'] = scope_data['fisc_wk_strt_dt'].dt.month\n",
    "scope_data['year'] = scope_data['fisc_wk_strt_dt'].dt.year\n",
    "scope_data['week_of_month'] = scope_data['fisc_wk_strt_dt'].apply(lambda x: (x.day-1) // 7 + 1)\n",
    "\n",
    "# Group by relevant columns and calculate aggregated metrics\n",
    "scope_data = (\n",
    "    scope_data.groupby(['fisc_wk_strt_dt', 'month', 'year', 'week_of_month', 'mkt_lvl', 'value'], as_index=False)\n",
    "    .agg(total_wgt=('total_wgt', 'sum'),total_sales=('total_sales', 'sum'))\n",
    "             )\n",
    "# Calculate additional metrics\n",
    "scope_data['avg_unit_price'] = scope_data['total_sales'] / scope_data['total_wgt']\n",
    "scope_data['avg_unit_price_adj'] = (scope_data['avg_unit_price'] / scope_data['value']) * 100\n",
    "scope_data['log_price'] = scope_data['avg_unit_price_adj'].apply(lambda x: math.log(x))\n",
    "scope_data['log_units'] = scope_data['total_wgt'].apply(lambda x: math.log(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86da6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit_elasticnet_tidymodels  <- function(data) {\n",
    "  \n",
    "  \n",
    "  \n",
    "#   elasticnet_spec <- linear_reg(penalty = tune(), mixture = 0.5) %>%\n",
    "#     set_engine(\"glmnet\") %>%\n",
    "#     set_mode(\"regression\")\n",
    "  \n",
    "#   ###sales_recipe <- recipe(log_units ~ log_price +  month + week_of_month + year, data = data) %>%\n",
    "#   sales_recipe <- recipe(log_units ~ log_price , data = data) %>%\n",
    "#     step_dummy(all_nominal_predictors(), -all_outcomes()) %>%\n",
    "#     step_normalize(all_numeric_predictors(), -all_outcomes()) %>% \n",
    "    \n",
    "#     step_zv(all_predictors()) \n",
    "  \n",
    "#   sales_recipe_prep <- prep(sales_recipe)\n",
    "#   sales_recipe_juiced <- juice(sales_recipe_prep)\n",
    "  \n",
    "  \n",
    "  \n",
    "  \n",
    "#   # Create a 10-fold cross-validation object\n",
    "#   sales_vfold <- vfold_cv(data, v = 10, strata = log_units)\n",
    "#   # Grid search for tuning the penalty parameter\n",
    "#   grid_vals <- tibble(penalty = 10^seq(-6, -1, length.out = 20))\n",
    "  \n",
    "  \n",
    "  \n",
    "#   # Perform cross-validated grid search\n",
    "#   elasticnet_tune <- tune_grid(elasticnet_spec,\n",
    "#                                preprocessor  = sales_recipe,\n",
    "#                                resamples = sales_vfold,\n",
    "#                                grid = grid_vals,\n",
    "#                                metrics = metric_set(rmse, rsq))\n",
    "  \n",
    "  \n",
    "  \n",
    "  \n",
    "#   # Extract the best model\n",
    "#   best_model <- elasticnet_tune %>%\n",
    "#     select_best(metric = \"rmse\")\n",
    "  \n",
    "  \n",
    "  \n",
    "  \n",
    "#   # Prepare the recipe for the whole dataset\n",
    "#   prepared_recipe <- prep(sales_recipe, training = data)\n",
    "#   baked_data <- bake(prepared_recipe, new_data = data)\n",
    "  \n",
    "  \n",
    "#   # Fit the best model on the whole dataset\n",
    "#   best_fit <- elasticnet_spec %>%\n",
    "#     finalize_model(best_model) %>%\n",
    "#     fit(log_units ~ ., data = baked_data)\n",
    "  \n",
    "  \n",
    "  \n",
    "#   # Extract the log_price and log_units_processed coefficient\n",
    "  \n",
    "#   coefficients <-  tidy(best_fit)\n",
    "#   recipe_tidied <- tidy(sales_recipe)\n",
    "  \n",
    "#   coefs_normalized <- coefficients %>%\n",
    "#     filter(term %in% c('log_price','log_units_processed')) %>%\n",
    "#     select(-penalty)\n",
    "  \n",
    "  \n",
    "  \n",
    "#   normalized_stats <- tidy(prepared_recipe, number = 2) %>%\n",
    "#     filter(terms %in% c('log_price')) %>%\n",
    "#     filter(statistic == 'sd') %>% select(-statistic) %>% \n",
    "#     rename(sd = value, term = terms) %>%\n",
    "#     select(-id)\n",
    "  \n",
    "  \n",
    "#   coefs_denormalized = coefs_normalized  %>% inner_join(normalized_stats) %>%\n",
    "#     mutate(beta_denormalized = estimate / sd)\n",
    "  \n",
    "#   log_price_beta = coefs_denormalized %>% filter(term == 'log_price') %>% pull(beta_denormalized)\n",
    "  \n",
    "  \n",
    "  \n",
    "#   result <- list('price_elasticity' = log_price_beta )\n",
    "  \n",
    "  \n",
    "#   return(result) }\n",
    "\n",
    "\n",
    "\n",
    "def fit_elasticnet_tidymodels(data):\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.linear_model import ElasticNetCV\n",
    "    \n",
    "    # Prepare the recipe\n",
    "    sales_recipe = pd.concat([data[['log_price']], pd.get_dummies(data.drop(['log_units', 'log_price'], axis=1))], axis=1)\n",
    "    sales_recipe = StandardScaler().fit_transform(sales_recipe)\n",
    "    \n",
    "    # Create an ElasticNet model\n",
    "    elasticnet_spec = ElasticNetCV(cv=10, l1_ratio=[0.5], alphas=[10**i for i in range(-6, 0)], max_iter=100000)\n",
    "    \n",
    "    # Fit the ElasticNet model\n",
    "    elasticnet_fit = elasticnet_spec.fit(sales_recipe, data['log_units'])\n",
    "    \n",
    "    # Extract the coefficients\n",
    "    coef_df = pd.DataFrame({'term': ['log_price'] + list(data.drop(['log_units', 'log_price'], axis=1).columns)})\n",
    "    coef_df['estimate'] = elasticnet_fit.coef_\n",
    "    coef_df['penalty'] = elasticnet_fit.alpha_\n",
    "    \n",
    "    # Calculate price elasticity\n",
    "    price_coef = coef_df.loc[coef_df['term'] == 'log_price', 'estimate'].values[0]\n",
    "    price_sd = sales_recipe[:, 0].std()\n",
    "    price_elasticity = price_coef * (data['log_price'].mean() / price_sd)\n",
    "    \n",
    "    # Create result dictionary\n",
    "    result = {'price_elasticity': price_elasticity}\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657f6a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scope_data <- na.omit(scope_data)\n",
    "\n",
    "scope_data = scope_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe6b929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# price_elasticities_tidymodels3 <- scope_data %>% filter(fisc_wk_strt_dt > \"2022-05-01\") %>%\n",
    "# group_by(mkt_lvl) %>% nest() %>% mutate(elasticities = purrr::map(data, fit_elasticnet_tidymodels)) \n",
    "\n",
    "\n",
    "price_elasticities_tidymodels3 = (scope_data.query(\"fisc_wk_strt_dt > '2022-05-01'\")\n",
    "                                         .groupby('mkt_lvl')\n",
    "                                         .apply(lambda x: fit_elasticnet_tidymodels(x))\n",
    "                                         .reset_index(name='elasticities'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebc7112",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_elasticnet_tidymodels  <- function(data) {\n",
    " \n",
    " \n",
    " \n",
    " \n",
    "  elasticnet_spec <- linear_reg(penalty = tune(), mixture = 0.5) %>%\n",
    "    set_engine(\"glmnet\") %>%\n",
    "    set_mode(\"regression\")\n",
    " \n",
    "  sales_recipe <- recipe(log_units ~ log_price + month + week_of_month + year, data = data) %>%\n",
    "  ###sales_recipe <- recipe(log_units ~ log_price , data = data) %>%\n",
    "    step_dummy(all_nominal_predictors(), -all_outcomes()) %>%\n",
    "    step_normalize(all_numeric_predictors(), -all_outcomes()) %>%\n",
    "   \n",
    "    step_zv(all_predictors())\n",
    "    \n",
    " \n",
    "  sales_recipe_prep <- prep(sales_recipe)\n",
    "  sales_recipe_juiced <- juice(sales_recipe_prep)\n",
    " \n",
    " \n",
    " \n",
    " \n",
    "  # Create a 10-fold cross-validation object\n",
    "  sales_vfold <- vfold_cv(data, v = 10, strata = log_units)\n",
    "  # Grid search for tuning the penalty parameter\n",
    "  grid_vals <- tibble(penalty = 10^seq(-6, -1, length.out = 20))\n",
    " \n",
    " \n",
    " \n",
    "  # Perform cross-validated grid search\n",
    "  elasticnet_tune <- tune_grid(elasticnet_spec,\n",
    "                               preprocessor  = sales_recipe,\n",
    "                               resamples = sales_vfold,\n",
    "                               grid = grid_vals,\n",
    "                               metrics = metric_set(rmse, rsq))\n",
    " \n",
    " \n",
    " \n",
    " \n",
    "  # Extract the best model\n",
    "  best_model <- elasticnet_tune %>%\n",
    "    select_best(metric = \"rmse\")\n",
    " \n",
    " \n",
    " \n",
    " \n",
    "  # Prepare the recipe for the whole dataset\n",
    "  prepared_recipe <- prep(sales_recipe, training = data)\n",
    "  baked_data <- bake(prepared_recipe, new_data = data)\n",
    " \n",
    " \n",
    "  # Fit the best model on the whole dataset\n",
    "  best_fit <- elasticnet_spec %>%\n",
    "    finalize_model(best_model) %>%\n",
    "    fit(log_units ~ ., data = baked_data)\n",
    " \n",
    " \n",
    " \n",
    "  # Extract the log_price and log_units_processed coefficient\n",
    " \n",
    "  coefficients <-  tidy(best_fit)\n",
    "  recipe_tidied <- tidy(sales_recipe)\n",
    " \n",
    "  coefs_normalized <- coefficients %>%\n",
    "    filter(term %in% c('log_price','log_units_processed')) %>%\n",
    "    select(-penalty)\n",
    " \n",
    " \n",
    " \n",
    "  normalized_stats <- tidy(prepared_recipe, number = 2) %>%\n",
    "    filter(terms %in% c('log_price')) %>%\n",
    "    filter(statistic == 'sd') %>% select(-statistic) %>%\n",
    "    rename(sd = value, term = terms) %>%\n",
    "    select(-id)\n",
    " \n",
    " \n",
    "  coefs_denormalized = coefs_normalized  %>% inner_join(normalized_stats) %>%\n",
    "    mutate(beta_denormalized = estimate / sd)\n",
    " \n",
    "  log_price_beta = coefs_denormalized %>% filter(term == 'log_price') %>% pull(beta_denormalized)\n",
    " \n",
    " \n",
    " \n",
    "  result <- list('price_elasticity' = log_price_beta )\n",
    " \n",
    " \n",
    "  return(result) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c454361c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import make_scorer, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def fit_elasticnet_sklearn(data):\n",
    "    \n",
    "    # Create the pipeline for the model\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('onehot', OneHotEncoder())\n",
    "    ])\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, ['log_price', 'month', 'week_of_month', 'year']),\n",
    "            ('cat', categorical_transformer, ['month', 'week_of_month', 'year'])\n",
    "        ])\n",
    "    elasticnet_spec = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('regressor', ElasticNet())\n",
    "    ])\n",
    "    \n",
    "    # Create the cross-validation object\n",
    "    sales_cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n",
    "    \n",
    "    # Grid search for tuning the penalty parameter\n",
    "    grid_vals = {'regressor__alpha': 10**np.linspace(-6, -1, num=20)}\n",
    "    \n",
    "    # Perform cross-validated grid search\n",
    "    elasticnet_tune = GridSearchCV(elasticnet_spec, grid_vals, cv=sales_cv,\n",
    "                                   scoring={'rmse': make_scorer(mean_squared_error, squared=False),\n",
    "                                            'rsq': make_scorer(r2_score)},\n",
    "                                   refit='rmse')\n",
    "    elasticnet_tune.fit(data[['log_price', 'month', 'week_of_month', 'year']], data['log_units'])\n",
    "    \n",
    "    # Extract the best model\n",
    "    best_model = elasticnet_tune.best_estimator_\n",
    "    \n",
    "    # Prepare the data for the whole dataset\n",
    "    prepared_data = preprocessor.fit_transform(data[['log_price', 'month', 'week_of_month', 'year']])\n",
    "    \n",
    "    # Fit the best model on the whole dataset\n",
    "    best_fit = best_model.fit(prepared_data, data['log_units'])\n",
    "    \n",
    "    # Extract the log_price and log_units_processed coefficient\n",
    "    coefficients = pd.DataFrame({'term': preprocessor.transformers_[1][1]['onehot'].get_feature_names(['month', 'week_of_month', 'year'])})\n",
    "    coefficients['estimate'] = np.concatenate((best_fit.named_steps['regressor'].coef_, np.zeros(preprocessor.transformers_[0][2],)))\n",
    "    coefficients['penalty'] = np.concatenate((np.zeros(len(best_fit.named_steps['regressor'].coef_)), best_fit.named_steps['regressor'].coef_))\n",
    "    coefs_normalized = coefficients.loc[coefficients['term'].isin(['log_price', 'log_units_processed']), ['term', 'estimate', 'penalty']]\n",
    "    \n",
    "    normalized_stats = pd.DataFrame({'term': ['log_price'], 'sd': [np.std(data['log_price'])]})\n",
    "    coefs_denormalized = pd.merge(coefs_normalized, normalized_stats, on='term')\n",
    "    coefs_denormalized['beta_denormalized'] = coefs_denormalized['estimate'] / coefs_denormalized['sd']\n",
    "    log_price_beta = coefs_denormalized.loc[coefs_denormalized['term'] == 'log_price', 'beta_denormalized'].iloc[0]\n",
    "    \n",
    "    result = {'price_elasticity': log_price_beta}\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc763103",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import GridSearchCV, RepeatedKFold\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def fit_elasticnet_sklearn(data):\n",
    "  \n",
    "  # Create a pipeline for preprocessing and modeling\n",
    "  numeric_transformer = Pipeline(steps=[\n",
    "      ('scaler', StandardScaler())])\n",
    "  \n",
    "  categorical_transformer = Pipeline(steps=[\n",
    "      ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "  \n",
    "  preprocessor = ColumnTransformer(\n",
    "      transformers=[\n",
    "          ('num', numeric_transformer, ['log_price', 'month', 'week_of_month', 'year']),\n",
    "          ('cat', categorical_transformer, ['month'])\n",
    "      ])\n",
    "  \n",
    "  elasticnet = ElasticNet(normalize=False)\n",
    "  \n",
    "  pipe = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                         ('elasticnet', elasticnet)])\n",
    "  \n",
    "  # Set up the grid search\n",
    "  param_grid = {'elasticnet__alpha': np.logspace(-6, -1, 20),\n",
    "                'elasticnet__l1_ratio': [0.5]}\n",
    "  \n",
    "  cv = RepeatedKFold(n_splits=10, n_repeats=1, random_state=0)\n",
    "  \n",
    "  grid_search = GridSearchCV(pipe, param_grid=param_grid, cv=cv,\n",
    "                             scoring=('neg_mean_squared_error', 'r2'),\n",
    "                             refit='neg_mean_squared_error')\n",
    "  \n",
    "  # Fit the grid search to the data\n",
    "  grid_search.fit(data[['log_price', 'month', 'week_of_month', 'year']], data['log_units'])\n",
    "  \n",
    "  # Extract the best model\n",
    "  best_model = grid_search.best_estimator_\n",
    "  \n",
    "  # Extract the log_price and log_units_processed coefficient\n",
    "  coefs = pd.DataFrame({'term': preprocessor.transformers_[0][2] + \n",
    "                                  list(preprocessor.transformers_[1][1].get_feature_names()),\n",
    "                        'estimate': best_model.named_steps['elasticnet'].coef_})\n",
    "  coefs['penalty'] = grid_search.best_params_['elasticnet__alpha']\n",
    "  \n",
    "  coefs_normalized = coefs.loc[coefs['term'].isin(['log_price', 'month'])].copy()\n",
    "  \n",
    "  normalized_stats = pd.DataFrame({'term': ['log_price'], \n",
    "                                    'sd': [np.std(data['log_price'])]})\n",
    "  \n",
    "  coefs_denormalized = pd.merge(coefs_normalized, normalized_stats, on='term', how='left')\n",
    "  coefs_denormalized['beta_denormalized'] = coefs_denormalized['estimate'] / coefs_denormalized['sd']\n",
    "  \n",
    "  log_price_beta = coefs_denormalized.loc[coefs_denormalized['term'] == 'log_price', 'beta_denormalized'].values[0]\n",
    "  \n",
    "  result = {'price_elasticity': log_price_beta}\n",
    "  \n",
    "  return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c0c395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scope_data <- na.omit(scope_data)\n",
    "\n",
    "# price_elasticities_tidymodels_data <- scope_data %>%\n",
    "#   mutate(season = case_when( month %in% c(3,4,5) ~ \"spring\",\n",
    "#                           month %in% c(6,7,8) ~ \"Summer\",\n",
    "#                           month %in% c(9,10,11) ~ \"Fall\",\n",
    "#                           TRUE ~ \"Winter\",\n",
    "#                            )) %>%\n",
    "#   filter(\n",
    "#     season == \"spring\") %>%\n",
    "#   group_by(mkt_lvl) %>%\n",
    "#     nest() %>%\n",
    " \n",
    "#   mutate(elasticities = purrr::map(data, fit_elasticnet_tidymodels))\n",
    "\n",
    "# price_elasticities_tidymodels = price_elasticities_tidymodels_spring %>%\n",
    "#   unnest_longer( elasticities) %>%\n",
    "#   select(mkt_lvl, elasticities, elasticities_id) %>%\n",
    "#   pivot_wider(names_from = elasticities_id , values_from = elasticities,values_fill  = 0)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# assuming that 'scope_data' is a pandas DataFrame object\n",
    "price_elasticities_tidymodels_data = (\n",
    "    scope_data.assign(season=np.select(\n",
    "        [scope_data['month'].isin([3, 4, 5]),\n",
    "         scope_data['month'].isin([6, 7, 8]),\n",
    "         scope_data['month'].isin([9, 10, 11])],\n",
    "        ['spring', 'Summer', 'Fall'], default='Winter'))\n",
    "    .query('season == \"spring\"')\n",
    "    .groupby('mkt_lvl')\n",
    "    .apply(lambda x: x[['price', 'qty']].values.tolist())\n",
    "    .reset_index(name='data')\n",
    "    .assign(elasticities=lambda x: x['data'].apply(fit_elasticnet_tidymodels))\n",
    ")\n",
    "\n",
    "price_elasticities_tidymodels = (\n",
    "    pd.json_normalize(price_elasticities_tidymodels_data,\n",
    "                      meta=['mkt_lvl', 'elasticities_id'],\n",
    "                      record_path='elasticities')\n",
    "    .pivot(index='mkt_lvl', columns='elasticities_id', values='elasticities')\n",
    "    .fillna(0)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc0e045",
   "metadata": {},
   "outputs": [],
   "source": [
    "##summer\n",
    "# price_elasticities_tidymodels_summer <- scope_data %>%\n",
    "#   mutate(season = case_when( month %in% c(3,4,5) ~ \"spring\",\n",
    "#                              month %in% c(6,7,8) ~ \"summer\",\n",
    "#                              month %in% c(9,10,11) ~ \"fall\",\n",
    "#                              TRUE ~ \"winter\",\n",
    "#   )) %>%\n",
    "#   filter(\n",
    "#     season == \"summer\") %>%\n",
    "#   group_by(mkt_lvl) %>%\n",
    "#   nest() %>%\n",
    " \n",
    "#   mutate(elasticities = purrr::map(data, fit_elasticnet_tidymodels))\n",
    "\n",
    "price_elasticities_tidymodels_summer = scope_data.copy()\n",
    "price_elasticities_tidymodels_summer['season'] = np.select(\n",
    "[price_elasticities_tidymodels_summer['month'].isin([3,4,5]),\n",
    "price_elasticities_tidymodels_summer['month'].isin([6,7,8]),\n",
    "price_elasticities_tidymodels_summer['month'].isin([9,10,11])\n",
    "],\n",
    "['spring', 'summer', 'fall'],\n",
    "default='winter'\n",
    ")\n",
    "price_elasticities_tidymodels_summer = (\n",
    "price_elasticities_tidymodels_summer[price_elasticities_tidymodels_summer['season'] == 'summer']\n",
    ".groupby('mkt_lvl').apply(lambda x: x.drop(['mkt_lvl', 'season'], axis=1))\n",
    ".pipe(lambda x: x.set_index('mkt_lvl', append=True))\n",
    ".pipe(lambda x: x.groupby(level=[0,1]).apply(lambda y: fit_elasticnet_sklearn(y)))\n",
    ".reset_index('mkt_lvl')\n",
    ".rename(columns={'mkt_lvl': 'elasticities_id', 0: 'elasticities'})\n",
    ".reset_index()\n",
    ".pivot_table(index=['mkt_lvl', 'elasticities_id'], columns='level_1', values='elasticities')\n",
    ".reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcce9df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# price_elasticities_tidymodels_summer = price_elasticities_tidymodels_summer %>%\n",
    "#   unnest_longer( elasticities) %>%\n",
    "#   select(mkt_lvl, elasticities, elasticities_id) %>%\n",
    "#   pivot_wider(names_from = elasticities_id , values_from = elasticities,values_fill  = 0)\n",
    "\n",
    "\n",
    "# # Mutate season column\n",
    "# scope_data['season'] = np.select(\n",
    "#     [\n",
    "#         scope_data['month'].isin([3, 4, 5]),\n",
    "#         scope_data['month'].isin([6, 7, 8]),\n",
    "#         scope_data['month'].isin([9, 10, 11])\n",
    "#     ],\n",
    "#     ['spring', 'summer', 'fall'],\n",
    "#     default='winter'\n",
    "# )\n",
    "\n",
    "# Filter data for summer\n",
    "price_elasticities_tidymodels_summer = (\n",
    "    scope_data\n",
    "    .query('season == \"summer\"')\n",
    "    .groupby('mkt_lvl')\n",
    "    .apply(lambda x: pd.DataFrame({'elasticities': tm.fit_elasticnet_tidymodels(x.drop(columns=['mkt_lvl', 'season']))}))\n",
    "    .reset_index()\n",
    "    .rename(columns={'level_1': 'elasticities_id'})\n",
    "    .explode('elasticities')\n",
    "    .pivot_table(index=['mkt_lvl', 'elasticities_id'], columns='level_2', values='elasticities', fill_value=0)\n",
    "    .reset_index()\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
